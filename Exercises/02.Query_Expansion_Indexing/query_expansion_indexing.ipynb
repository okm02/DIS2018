{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Query Expansion and Indexing\n",
    "\n",
    "The following code is modified from Exercise 1. It is used to construct the vocabulary and vectorize the documents and query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/omar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "import numpy as np\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Tokenize, stem a document\n",
    "def tokenize(text):\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation])\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return \" \".join([stemmer.stem(word.lower()) for word in tokens])\n",
    "\n",
    "# Read a list of documents from a file. Each line in a file is a document\n",
    "with open(\"epfldocs.txt\") as f:\n",
    "# with open(\"epfldocs.txt\") as f:\n",
    "    content = f.readlines()\n",
    "original_documents = [x.strip() for x in content] \n",
    "documents = [tokenize(d).split() for d in original_documents]\n",
    "\n",
    "# create the vocabulary\n",
    "vocabulary = set([item for sublist in documents for item in sublist])\n",
    "vocabulary = [word for word in vocabulary if word not in stopwords.words('english')]\n",
    "vocabulary.sort()\n",
    "\n",
    "# compute IDF, storing idf values in a dictionary\n",
    "def idf_values(vocabulary, documents):\n",
    "    idf = {}\n",
    "    num_documents = len(documents)\n",
    "    for i, term in enumerate(vocabulary):\n",
    "        idf[term] = math.log(num_documents/sum(term in document for document in documents), math.e)\n",
    "    return idf\n",
    "\n",
    "# Function to generate the vector for a document (with normalisation)\n",
    "def vectorize(document, vocabulary, idf):\n",
    "    vector = [0]*len(vocabulary)\n",
    "    counts = Counter(document)\n",
    "    max_count = counts.most_common(1)[0][1]\n",
    "    for i,term in enumerate(vocabulary):\n",
    "        vector[i] = idf[term] * counts[term]/max_count\n",
    "    return vector\n",
    "\n",
    "# Function to compute cosine similarity\n",
    "def cosine_similarity(v1,v2):\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    if sumxy == 0:\n",
    "            result = 0\n",
    "    else:\n",
    "            result = sumxy/math.sqrt(sumxx*sumyy)\n",
    "    return result\n",
    "\n",
    "def vectorize_query(query, vocabulary, idf):\n",
    "    q = query.split()\n",
    "    q = [stemmer.stem(w) for w in q]\n",
    "    query_vector = vectorize(q, vocabulary, idf)\n",
    "    return query_vector\n",
    "    \n",
    "def search_vec(query, k):\n",
    "    query_vector = vectorize_query(query, vocabulary, idf)\n",
    "    scores = [[cosine_similarity(query_vector, document_vectors[d]), d] for d in range(len(documents))]\n",
    "    scores.sort(key=lambda x: -x[0])\n",
    "    ans = []\n",
    "    indices = []\n",
    "    for i in range(k):\n",
    "        ans.append(original_documents[scores[i][1]])\n",
    "        indices.append(scores[i][1])\n",
    "    return ans, indices, query_vector\n",
    "\n",
    "# Compute IDF values and vectors\n",
    "idf = idf_values(vocabulary, documents)\n",
    "document_vectors = [vectorize(s, vocabulary, idf) for s in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 - Relevance Feedback\n",
    "\n",
    "In this exercise we will implement and test Rocchio's method for user relevance feedback.\n",
    "\n",
    "Let the set of relevant documents to be $D_r $ and the set of non-relevant documents to be $D_{nr}$. Then the modified query  $\\vec{q_m}$  according to the Rocchio method is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{q_m} = \\alpha \\vec{q_0} + \\frac{\\beta}{|D_r|} \\sum_{\\vec{d_j} \\in D_r} \\vec{d_j} - \\frac{\\gamma}{|D_{nr}|} \\sum_{\\vec{d_j} \\in D_{nr}} \\vec{d_j}\n",
    "\\end{equation}\n",
    "In the Rocchio algorithm negative term weights are ignored. This means, for the negative term weights in $\\vec{q_m}$, we set them to be 0.\n",
    "\n",
    "First, complete the implementation of the Rocchio relevance feedback method, by adding the missing code for the function $expand\\_query$.   \n",
    "\n",
    "Then, compare the result obtained with the new query with the unmodified one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_query(relevant_doc_vecs, non_relevant_doc_vecs, query_vector, alpha, beta, gamma):\n",
    "    \n",
    "    num_rel = len(relevant_doc_vecs)\n",
    "    num_non_rel = len(non_relevant_doc_vecs)\n",
    "    \n",
    "    # Compute the first term in the Rocchio equation\n",
    "    norm_query_vector = alpha * query_vector\n",
    "    norm_query_vector = np.asarray(norm_query_vector)\n",
    "    \n",
    "    # Compute the second term in the Rocchio equation\n",
    "   \n",
    "    sum_relevant = list(map(lambda elem : np.asarray(elem),relevant_doc_vecs))\n",
    "    sum_relevant = np.asarray(sum_relevant)\n",
    "    sum_relevant = np.sum(sum_relevant,axis = 0) \n",
    "    norm_sum_relevant = (beta/num_rel) * sum_relevant\n",
    "    \n",
    "    \n",
    "     # Compute the second term in the Rocchio equation\n",
    "   \n",
    "    sum_non_relevant = list(map(lambda elem : np.asarray(elem),non_relevant_doc_vecs))\n",
    "    sum_non_relevant = np.asarray(sum_non_relevant)\n",
    "    sum_non_relevant = np.sum(sum_non_relevant,axis = 0) \n",
    "    norm_sum_non_relevant = (gamma/num_rel) * sum_non_relevant\n",
    "   \n",
    "    \n",
    "    # Sum all the terms\n",
    "    modified_query_vector = np.add(np.add(norm_query_vector,norm_sum_relevant),norm_sum_non_relevant)\n",
    "    \n",
    "    # Ignore negative elements\n",
    "    modified_query_vector = [x if x>0 else 0 for x in modified_query_vector]\n",
    "    \n",
    "    return modified_query_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will give you the result for the query \"computer science\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Exciting News: \"World University Rankings 2016-2017 by subject: computer science\" No1 @ETH &amp; @EPFL on No8. Congrats https://t.co/ARSlXZoShQ\n",
      "1 New computer model shows how proteins are controlled \"at a distance\" https://t.co/zNjK3bZ6mO  via @EPFL_en #VDtech https://t.co/b9TglXO4KD\n",
      "2 An interview with Patrick Barth, a new @EPFL professor who combines protein #biophysics with computer modeling https://t.co/iJwBaEbocj\n",
      "3 New at @epfl_en Life Sciences @epflSV: \"From PhD directly to Independent Group Leader\" #ELFIR_EPFL:  Early Independence Research Scholars. See https://t.co/evqyqD7FFl, also for computational biology #compbio https://t.co/e3pDCg6NVb Deadline April 1 2018 at https://t.co/mJqcrfIqkb\n",
      "4 Video of Nicola Marzari from @EPFL_en  on Computational Discovery in the 21st Century during #PASC17 now online: https://t.co/tfCkEvYKtq https://t.co/httPdHcK9W\n",
      "5 Exposure Science Film Hackathon 2017 applications open! Come join our Scicomm-film-hacking event! #Science #scicomm https://t.co/zwtKPlh6HT\n",
      "6 @CodeWeekEU is turning 5, yay! We look very much forward to computational thinking unplugged activities during @CodeWeek_CH https://t.co/yDPrlKg4hw\n",
      "7 Le mystère Soulages éblouit la science @EPFL  https://t.co/u3uNICyAdi\n",
      "8 @cwarwarrior @EPFL_en @EPFL Doing science at @EPFL_en is indeed pretty cool!!! Thank you for visiting!!!\n",
      "9 Blue Brain Nexus: an open-source tool for data-driven science https://t.co/m5yTgXf7ym #epfl\n"
     ]
    }
   ],
   "source": [
    "ans, result_doc_ids, query_vector = search_vec(\"computer science\", 10)\n",
    "for i in range(len(ans)):\n",
    "    print(i,ans[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code produces the result for the unmodified query (example: \"computer science\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified query:  1 20162017 2017 2018 21st 5 activ also amp applic april barth biolog biophys blue brain centuri codeweekch codeweekeu combin come compbio comput congrat control cool cwarwarrior datadriven deadlin directli discoveri distanc dure earli elfirepfl epfl epflen epflsv eth event excit exposur film forward group hackathon httpstcoarslxzoshq httpstcob9tglxo4kd httpstcoe3pdcg6nvb httpstcoevqyqd7ffl httpstcohttpdhck9w httpstcoijwbaebocj httpstcom5ytgxf7ym httpstcomjqcrfiqkb httpstcotfckevyktq httpstcou3unicyadi httpstcoydprlkg4hw httpstcoznjk3bz6mo httpstcozwtkplh6ht inde independ interview join la le leader life look marzari model much mystèr new news nexu nicola no1 no8 onlin open opensourc pasc17 patrick phd pretti professor protein rank research scholar scicomm scicommfilmhack scienc see show soulag subject thank think tool turn univers unplug vdtech veri via video visit world yay éblouit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['New at @epfl_en Life Sciences @epflSV: \"From PhD directly to Independent Group Leader\" #ELFIR_EPFL:  Early Independence Research Scholars. See https://t.co/evqyqD7FFl, also for computational biology #compbio https://t.co/e3pDCg6NVb Deadline April 1 2018 at https://t.co/mJqcrfIqkb',\n",
       " '@CodeWeekEU is turning 5, yay! We look very much forward to computational thinking unplugged activities during @CodeWeek_CH https://t.co/yDPrlKg4hw',\n",
       " 'Video of Nicola Marzari from @EPFL_en  on Computational Discovery in the 21st Century during #PASC17 now online: https://t.co/tfCkEvYKtq https://t.co/httPdHcK9W',\n",
       " 'Exciting News: \"World University Rankings 2016-2017 by subject: computer science\" No1 @ETH &amp; @EPFL on No8. Congrats https://t.co/ARSlXZoShQ',\n",
       " 'Exposure Science Film Hackathon 2017 applications open! Come join our Scicomm-film-hacking event! #Science #scicomm https://t.co/zwtKPlh6HT',\n",
       " 'New computer model shows how proteins are controlled \"at a distance\" https://t.co/zNjK3bZ6mO  via @EPFL_en #VDtech https://t.co/b9TglXO4KD',\n",
       " 'An interview with Patrick Barth, a new @EPFL professor who combines protein #biophysics with computer modeling https://t.co/iJwBaEbocj',\n",
       " 'Blue Brain Nexus: an open-source tool for data-driven science https://t.co/m5yTgXf7ym #epfl',\n",
       " 'Le mystère Soulages éblouit la science @EPFL  https://t.co/u3uNICyAdi',\n",
       " '@cwarwarrior @EPFL_en @EPFL Doing science at @EPFL_en is indeed pretty cool!!! Thank you for visiting!!!']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of indices marked as relevant\n",
    "\n",
    "relevant_indices = [1,2]\n",
    "\n",
    "# remove relevant indices from all indices\n",
    "non_relevant_indices =  [n for i, n in enumerate([l for l in range(len(ans))]) if i not in relevant_indices]\n",
    "\n",
    "def items_from_list(indices, alist):\n",
    "    from operator import itemgetter\n",
    "    return list(itemgetter(*indices)(alist))\n",
    "\n",
    "relevant_doc_ids = items_from_list(relevant_indices, result_doc_ids)\n",
    "non_relevant_doc_ids = items_from_list(non_relevant_indices, result_doc_ids)\n",
    "\n",
    "relevant_doc_vecs = items_from_list(relevant_doc_ids, document_vectors)\n",
    "non_relevant_doc_vecs = items_from_list(non_relevant_doc_ids, document_vectors)\n",
    "\n",
    "\n",
    "\n",
    "expanded_query = expand_query(relevant_doc_vecs, non_relevant_doc_vecs, query_vector, 1, 1, 1)\n",
    "\n",
    "new_query = ' '.join([vocabulary[i] for i, val in enumerate(expanded_query) if val>0])\n",
    "\n",
    "new_ans , _, _ = search_vec(new_query, 10)\n",
    "\n",
    "print('Modified query: ', new_query)\n",
    "new_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 -  Distributed Information Retrieval\n",
    "\n",
    "In this exercise we implement a core process of Fagin's algorithm, the parallel scanning of the posting lists. Assume an aggregation function that returns the sum of the tf-idf scores given the terms in a document.\n",
    "\n",
    "We assume that the posting lists for each term of a retrieval system are running on a different node.\n",
    "\n",
    "We first create a dictionary $h$, where each entry of the dictionary corresponds to a posting list for term, assumed to be hosted in a different node. Explore the structure of $h$, to understand it. We suggest to first use the simple collection breads.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Tokenize, stem a document\n",
    "def tokenize(text):\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation])\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return \" \".join([stemmer.stem(word.lower()) for word in tokens])\n",
    "\n",
    "# Read a list of documents from a file. Each line in a file is a document\n",
    "with open(\"bread.txt\") as f:\n",
    "# with open(\"epfldocs.txt\") as f:\n",
    "    content = f.readlines()\n",
    "original_documents = [x.strip() for x in content] \n",
    "documents = [tokenize(d).split() for d in original_documents]\n",
    "\n",
    "# create the vocabulary\n",
    "vocabulary = set([item for sublist in documents for item in sublist])\n",
    "vocabulary = [word for word in vocabulary if word not in stopwords.words('english')]\n",
    "vocabulary.sort()\n",
    "\n",
    "# Compute IDF values and vectors\n",
    "idf = idf_values(vocabulary, documents)\n",
    "document_vectors = [vectorize(s, vocabulary, idf) for s in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "doc_vecs = np.transpose(np.array(document_vectors))\n",
    "h = {}\n",
    "for i, term in enumerate(vocabulary):\n",
    "    ha = {}\n",
    "    for docj in range(len(original_documents)):\n",
    "        tfidf = doc_vecs[i][docj]\n",
    "        ha[docj] = tfidf\n",
    "    sorted_ha = sorted(ha.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    h[term] = sorted_ha\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete the following function that implements the Fagin algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: 2, 2: 2}\n",
      "[(3, 1.1394342831883648), (2, 0.22314355131420976)]\n",
      "Breads, Pastries, Pies, and Cakes: Quantity Baking Recipes\n",
      "Numerical Recipes: The Art of Scientific Computing\n"
     ]
    }
   ],
   "source": [
    "def fagin_algorithm(query, h, k, vocabulary):\n",
    "    \n",
    "    # Split and stem the query\n",
    "    q = query.split()\n",
    "    q = set([stemmer.stem(w) for w in q])\n",
    "    query_term_cnt = len(q)\n",
    "    \n",
    "    # select the posting lists for the query terms\n",
    "    posting_lists = {}\n",
    "    for term in q:\n",
    "        if term in h:\n",
    "            posting_lists[term] = h[term]\n",
    "    \n",
    "    max_len = len(documents)\n",
    "    \n",
    "    # Traverse the selected posting lists until we found k documents that appear in ALL posting lists\n",
    "    # This corresponds to phase 1 of Fagin's algorithm.\n",
    "    # As a result you produce a dictionary documents_occurrences, with the document identifiers as keys, \n",
    "    # and the number of documents found as value.\n",
    "    # We stop traversing the posting lists until we have found k documents that appear in ALL posting lists \n",
    "    # of the query terms\n",
    "    \n",
    "    documents_occurrences = {}\n",
    "\n",
    "    count = 0\n",
    "    index =0\n",
    "    while(True):\n",
    "        for term in posting_lists:\n",
    "            current_position = posting_lists[term][index]\n",
    "            file_index = current_position[0]\n",
    "            if file_index not in documents_occurrences:\n",
    "                documents_occurrences[file_index] = 1\n",
    "            else:\n",
    "                documents_occurrences[file_index] += 1\n",
    "                if(documents_occurrences[file_index] == len(posting_lists)):\n",
    "                    count = count + 1\n",
    "        if(count == k):\n",
    "            break\n",
    "    \n",
    "              \n",
    "    print(documents_occurrences)\n",
    "        \n",
    "    # Retrieve for the found documents the tfidf values and add them up.\n",
    "    # For simplicity, we do not distinguish the cases whether the values have already been retrieved in the \n",
    "    # previous phase, or whether we use random access to obtain those values\n",
    "    \n",
    "    tfidf = {}\n",
    "    for d in documents_occurrences.keys():\n",
    "        t = 0\n",
    "        for term in q:\n",
    "            t = t + dict(posting_lists[term])[d]\n",
    "        tfidf[d] = t\n",
    "        \n",
    "    # Finally we compute the top-k documents and return the answer\n",
    "    \n",
    "    ans = sorted(tfidf.items(), key=lambda x:x[1], reverse = True)[:k]\n",
    "    return ans\n",
    "    \n",
    "\n",
    "ans = fagin_algorithm(\"bread recipe\", h, 2, vocabulary)\n",
    "print(ans)\n",
    "for document_id in ans:\n",
    "    print(original_documents[document_id[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce your own datasets to explore the behavior of the algorithm. Create a dataset such that for retrieving a 2 term query a total of 6 documents needs to be retrieved in the first phase of the algorithm (as in the example in the lecture)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 - Inverted Files (Hard)\n",
    "\n",
    "We learnt in the lecture that terms are typically stored in an inverted list. Now, in the inverted list, instead of only storing document identifiers of the documents in which the term appears, assume we also store an *offset* of the appearance of a term in a document. An $offset$ of a term $l_k$ given a document is defined as the number of words between the start of the document and $l_k$. Thus our inverted list is now:\n",
    "\n",
    "$l_k= \\langle f_k: \\{d_{i_1} \\rightarrow [o_1,\\ldots,o_{n_{i_1}}]\\}, \n",
    "\\{d_{i_2} \\rightarrow [o_1,\\ldots,o_{n_{i_2}}]\\}, \\ldots, \n",
    "\\{d_{i_k} \\rightarrow [o_1,\\ldots,o_{n_{i_k}}]\\} \\rangle$\n",
    "\n",
    "This means that in document $d_{i_1}$ term $l_k$ appears $n_{i_1}$ times and at offset $[o_1,,o_{n_{i_1}}]$, where $[o_1,,o_{n_{i_1}}]$ are sorted in ascending order, these type of indices are also known as term-offset indices. An example of a term-offset index is as follows:\n",
    "\n",
    "**Obama** = $⟨4 : {1 → [3]},{2 → [6]},{3 → [2,17]},{4 → [1]}⟩$\n",
    "\n",
    "**Governor** = $⟨2 : {4 → [3]}, {7 → [14]}⟩$\n",
    "\n",
    "**Election** = $⟨4 : {1 → [1]},{2 → [1,21]},{3 → [3]},{5 → [16,22,51]}⟩$\n",
    "\n",
    "Which is to say that the term **Governor** appear in 2 documents. In document 4 at offset 3, in document 7 at offset 14. Now let us consider the *SLOP/x* operator in text retrieval. This operator has the syntax: *QueryTerm1 SLOP/x QueryTerm2* finds occurrences of *QueryTerm1* within $x$ (but not necessarily in that order) words of *QueryTerm2*, where $x$ is a positive integer argument ($x \\geq 1$). Thus $x = 1$ demands that *QueryTerm1* be adjacent to *QueryTerm2*.\n",
    "\n",
    "1. List the set of documents that satisfy the query **Obama** *SLOP/2* **Election**.\n",
    "2. List each set of values for which the query **Obama** *SLOP/x* **Election** has a different set of documents as answers (starting from $x = 1$). \n",
    "3. Consider the general procedure for \"merging\" two term-offset inverted lists for a given document, to determine where the document satisfies a *SLOP/x* clause (since in general there will be many offsets at which each term occurs in a document). Let $L$ denote the total number of occurrences of the two terms in the document. Assume we have a pointer to the list of occurrences of each term and can move the pointer along this list. As we do so we check whether we have a hit for $SLOP/x$ (i.e. the $SLOP/x$ clause is satisfied). Each move of either pointer counts as a step. Based on this assumption is there a general \"merging\" procedure to determine whether the document satisfies a $SLOP/x$ clause, for which the following is true? Justify your answer.\n",
    "\n",
    "    1. The merge can be accomplished in a number of steps linear in $L$ regardless of $x$, and we can ensure that each pointer moves only to the right (i.e. forward).\n",
    "    2. The merge can be accomplished in a number of steps linear in $L$, but a pointer may be forced to move to the left (i.e. backwards).\n",
    "    3. The merge can require $x \\times L$ steps in some cases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
